{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages I will need\n",
    "from lex_processing import * \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# To show plots in notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path is where the data I want to process is.\n",
    "pathReagan = '/Users/Joe/Documents/PhD/data/edited data/presidents data/ReaganSpeeches/'\n",
    "pathBush = '/Users/Joe/Documents/PhD/data/edited data/presidents data/BushSpeeches/'\n",
    "pathTrump = '/Users/Joe/Documents/PhD/data/edited data/presidents data/TrumpSpeeches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Frames for the datasets. I am including one data frame for 2 terms of Reagan\n",
    "# and 2 separate dataframes for each term.\n",
    "dfReagan = pd.DataFrame()\n",
    "dfReaganTerm1 = pd.DataFrame()\n",
    "dfReaganTerm2 = pd.DataFrame()\n",
    "dfBush = pd.DataFrame()\n",
    "dfTrump = pd.DataFrame()\n",
    "LIWC = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathReagan):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathReagan + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('—')\n",
    "        wordsNoPunct.replace(\"—\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        words = word_tokenize(raw)\n",
    "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)),\n",
    "                    'UniqueStems':len(set(tokens_stemmed)),\n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfReagan = dfReagan.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathBush):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathBush + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('-')\n",
    "        wordsNoPunct.replace(\"-\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        \n",
    "        words = word_tokenize(raw)\n",
    "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)),\n",
    "                    'UniqueStems':len(set(tokens_stemmed)),\n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfBush = dfBush.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathTrump):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathTrump + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('-')\n",
    "        wordsNoPunct.replace(\"-\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        \n",
    "        words = word_tokenize(raw)\n",
    "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)),\n",
    "                    'UniqueStems':len(set(tokens_stemmed)),\n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfTrump = dfTrump.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac\n",
    "testpath = '/Users/Joe/Documents/PhD/NLP/'\n",
    "# For Linux\n",
    "# testpath = '/home/CAMPUS/alcantaj/Documents/NLP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC = pd.read_csv(testpath + \"LIWC2015Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan = pd.merge(dfReagan, LIWC, on='Filename', how='inner')\n",
    "dfTrump = pd.merge(dfTrump, LIWC, on='Filename', how='inner')\n",
    "dfBush = pd.merge(dfBush, LIWC, on='Filename', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'UniqueStems', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfReagan] \n",
    "        + [col for col in dfReagan if col not in inserted_cols])\n",
    "dfReagan = dfReagan[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'UniqueStems', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfBush] \n",
    "        + [col for col in dfBush if col not in inserted_cols])\n",
    "dfBush = dfBush[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'UniqueStems', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfTrump] \n",
    "        + [col for col in dfTrump if col not in inserted_cols])\n",
    "dfTrump = dfTrump[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA's with 0s as in this dataset, NAN represent the feature NOT occuring in a particular document.\n",
    "dfReagan = dfReagan.fillna(0)\n",
    "dfBush = dfBush.fillna(0)\n",
    "dfTrump = dfTrump.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output \n",
    "dfReagan.to_csv('testReagan.csv')\n",
    "dfBush.to_csv('testBush.csv')\n",
    "dfTrump.to_csv('testTrump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(testpath + \"dates.csv\")\n",
    "b = pd.read_csv(testpath + \"testReagan.csv\")\n",
    "\n",
    "c = pd.read_csv(testpath + \"dates2.csv\")\n",
    "d = pd.read_csv(testpath + \"testBush.csv\")\n",
    "\n",
    "e = pd.read_csv(testpath + \"dates3.csv\")\n",
    "f = pd.read_csv(testpath + \"testTrump.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in a['Date']]\n",
    "c['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in c['Date']]\n",
    "e['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in e['Date']]\n",
    "da = datetime.datetime(1981, 1, 29)\n",
    "da = np.datetime64(da)\n",
    "dc = datetime.datetime(1989, 1, 27)\n",
    "dc = np.datetime64(dc)\n",
    "de = datetime.datetime(2017, 1, 27)\n",
    "de = np.datetime64(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Days'] = a['JDate'] - da\n",
    "a['Days'] = a['Days'].dt.days\n",
    "c['Days'] = c['JDate'] - dc\n",
    "c['Days'] = c['Days'].dt.days\n",
    "e['Days'] = e['JDate'] - de\n",
    "e['Days'] = e['Days'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Julian'] = [get_julian_datetime(x) for x in a['JDate']]\n",
    "c['Julian'] = [get_julian_datetime(x) for x in c['JDate']]\n",
    "e['Julian'] = [get_julian_datetime(x) for x in e['JDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan = a.merge(b, on='Filename')\n",
    "dfBush = c.merge(d, on='Filename')\n",
    "dfTrump = e.merge(f, on='Filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan = dfReagan.sort_values(by=['Days'])\n",
    "dfBush = dfBush.sort_values(by=['Days'])\n",
    "dfTrump = dfTrump.sort_values(by=['Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new labels (Index is in order of article date)\n",
    "dfReagan = dfReagan.reset_index()\n",
    "dfReagan['index'] = dfReagan.index\n",
    "dfBush = dfBush.reset_index()\n",
    "dfBush['index'] = dfBush.index\n",
    "dfTrump = dfTrump.reset_index()\n",
    "dfTrump['index'] = dfTrump.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Redundant Columns\n",
    "dfReagan = dfReagan.drop(['Unnamed: 0'], axis=1)\n",
    "dfBush = dfBush.drop(['Unnamed: 0'], axis=1)\n",
    "dfTrump = dfTrump.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfReagan['Nouns'] = dfReagan['NN'] + dfReagan['NNS']+ dfReagan['NNP'] + dfReagan['NNPS']\n",
    "dfReagan['Nouns/100'] = dfReagan['Nouns'] / 100\n",
    "dfReagan['NounsNormalised'] = dfReagan['Nouns'] / dfReagan['WordCount']\n",
    "dfReagan['Adjectives'] = dfReagan['JJ'] + dfReagan['JJR'] + dfReagan['JJS']\n",
    "dfReagan['Adjectives/100'] = dfReagan['Adjectives'] / 100\n",
    "dfReagan['AdjectivesNormalised'] = dfReagan['Adjectives'] / dfReagan['WordCount']\n",
    "dfReagan['Adverbs'] = dfReagan['RB'] + dfReagan['RBR'] + dfReagan['RBS']\n",
    "dfReagan['Adverbs/100'] = dfReagan['Adverbs'] / 100\n",
    "dfReagan['AdverbsNormalised'] = dfReagan['Adverbs'] / dfReagan['WordCount']\n",
    "dfReagan['Verbs'] = dfReagan['VB'] + dfReagan['VBD'] + dfReagan['VBG'] + dfReagan['VBN'] + dfReagan['VBP'] + dfReagan['VBZ']\n",
    "dfReagan['Verbs/100'] = dfReagan['Verbs'] / 100\n",
    "dfReagan['VerbsNormalised'] = dfReagan['Verbs'] / dfReagan['WordCount']\n",
    "dfReagan['Pronouns'] = dfReagan['PRP'] + dfReagan['PRP$']\n",
    "dfReagan['PronounsNormalised'] = dfReagan['Pronouns'] / dfReagan['WordCount']\n",
    "dfReagan['UniqueWordsNormalised'] = dfReagan['UniqueWords'] / dfReagan['WordCount']\n",
    "dfReagan['UniqueStemsNormalised'] = dfReagan['UniqueStems'] / dfReagan['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfBush['Nouns'] = dfBush['NN'] + dfBush['NNS']+ dfBush['NNP'] + dfBush['NNPS']\n",
    "dfBush['Nouns/100'] = dfBush['Nouns'] / 100\n",
    "dfBush['NounsNormalised'] = dfBush['Nouns'] / dfBush['WordCount']\n",
    "dfBush['Adjectives'] = dfBush['JJ'] + dfBush['JJR'] + dfBush['JJS']\n",
    "dfBush['Adjectives/100'] = dfBush['Adjectives'] / 100\n",
    "dfBush['AdjectivesNormalised'] = dfBush['Adjectives'] / dfBush['WordCount']\n",
    "dfBush['Adverbs'] = dfBush['RB'] + dfBush['RBR'] + dfBush['RBS']\n",
    "dfBush['Adverbs/100'] = dfBush['Adverbs'] / 100\n",
    "dfBush['AdverbsNormalised'] = dfBush['Adverbs'] / dfBush['WordCount']\n",
    "dfBush['Verbs'] = dfBush['VB'] + dfBush['VBD'] + dfBush['VBG'] + dfBush['VBN'] + dfBush['VBP'] + dfBush['VBZ']\n",
    "dfBush['Verbs/100'] = dfBush['Verbs'] / 100\n",
    "dfBush['VerbsNormalised'] = dfBush['Verbs'] / dfBush['WordCount']\n",
    "dfBush['Pronouns'] = dfBush['PRP'] + dfBush['PRP$']\n",
    "dfBush['PronounsNormalised'] = dfBush['Pronouns'] / dfBush['WordCount']\n",
    "dfBush['UniqueWordsNormalised'] = dfReagan['UniqueWords'] / dfReagan['WordCount']\n",
    "dfBush['UniqueStemsNormalised'] = dfReagan['UniqueStems'] / dfReagan['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfTrump['Nouns'] = dfTrump['NN'] + dfTrump['NNS']+ dfTrump['NNP'] + dfTrump['NNPS']\n",
    "dfTrump['Nouns/100'] = dfTrump['Nouns'] / 100\n",
    "dfTrump['NounsNormalised'] = dfTrump['Nouns'] / dfTrump['WordCount']\n",
    "dfTrump['Adjectives'] = dfTrump['JJ'] + dfTrump['JJR'] + dfTrump['JJS']\n",
    "dfTrump['Adjectives/100'] = dfTrump['Adjectives'] / 100\n",
    "dfTrump['AdjectivesNormalised'] = dfTrump['Adjectives'] / dfTrump['WordCount']\n",
    "dfTrump['Adverbs'] = dfTrump['RB'] + dfTrump['RBR'] + dfTrump['RBS']\n",
    "dfTrump['Adverbs/100'] = dfTrump['Adverbs'] / 100\n",
    "dfTrump['AdverbsNormalised'] = dfTrump['Adverbs'] / dfTrump['WordCount']\n",
    "dfTrump['Verbs'] = dfTrump['VB'] + dfTrump['VBD'] + dfTrump['VBG'] + dfTrump['VBN'] + dfTrump['VBP'] + dfTrump['VBZ']\n",
    "dfTrump['Verbs/100'] = dfTrump['Verbs'] / 100\n",
    "dfTrump['VerbsNormalised'] = dfTrump['Verbs'] / dfTrump['WordCount']\n",
    "dfTrump['Pronouns'] = dfTrump['PRP'] + dfTrump['PRP$']\n",
    "dfTrump['PronounsNormalised'] = dfTrump['Pronouns'] / dfTrump['WordCount']\n",
    "dfTrump['UniqueWordsNormalised'] = dfReagan['UniqueWords'] / dfReagan['WordCount']\n",
    "dfTrump['UniqueStemsNormalised'] = dfReagan['UniqueStems'] / dfReagan['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReaganTerm1 = dfReagan[dfReagan.JDate < pd.Timestamp(1985, 1, 31)]\n",
    "dfReaganTerm2 = dfReagan[dfReagan.JDate > pd.Timestamp(1985, 1, 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReaganTerm1 = dfReaganTerm1.reset_index()\n",
    "dfReaganTerm1['index'] = dfReaganTerm1.index\n",
    "dfReaganTerm2 = dfReaganTerm2.reset_index()\n",
    "dfReaganTerm2['index'] = dfReaganTerm2.index\n",
    "\n",
    "dfReaganTerm1 = dfReaganTerm1.drop(['level_0'], axis=1)\n",
    "dfReaganTerm2 = dfReaganTerm2.drop(['level_0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Target Variable\n",
    "yReagan = dfReagan['index']\n",
    "yReaganTerm1 = dfReaganTerm1['index']\n",
    "yReaganTerm2 = dfReaganTerm2['index']\n",
    "yBush = dfBush['index']\n",
    "yTrump = dfTrump['index']\n",
    "\n",
    "#yRR = dfRR['Julian']\n",
    "#yGWHB = dfGWHB['Julian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to csv files\n",
    "dfReagan.to_csv('/Users/Joe/Documents/PhD/NLP/Data/Reagan.csv')\n",
    "dfBush.to_csv('/Users/Joe/Documents/PhD/NLP/Data/Bush.csv')\n",
    "dfTrump.to_csv('/Users/Joe/Documents/PhD/NLP/Data/Trump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
