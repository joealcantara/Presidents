{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages I will need\n",
    "from lex_processing import * \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To show plots in notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path is where the data I want to process is.\n",
    "# For Mac\n",
    "#pathReagan = '/Users/Joe/dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "# pathBush = '/Users/Joe/dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "#pathTrump = '/Users/Joe/dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'\n",
    "# For Linux\n",
    "#pathReagan = '/home/CAMPUS/alcantaj/Dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "#pathBush = '/home/CAMPUS/alcantaj/Dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "#pathTrump = '/home/CAMPUS/alcantaj/Dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'\n",
    "# For Windows\n",
    "pathReagan = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "pathBush = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "pathTrump = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Frames for the datasets. I am including one data frame for 2 terms of Reagan\n",
    "# and 2 separate dataframes for each term.\n",
    "\n",
    "dfReagan = pd.DataFrame()\n",
    "dfReaganTerm1 = pd.DataFrame()\n",
    "dfReaganTerm2 = pd.DataFrame()\n",
    "dfBush = pd.DataFrame()\n",
    "dfTrump = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathReagan):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathReagan + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('—')\n",
    "        wordsNoPunct.replace(\"—\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        words = word_tokenize(raw)\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)), \n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfReagan = dfReagan.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathBush):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathBush + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('-')\n",
    "        wordsNoPunct.replace(\"-\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        \n",
    "        words = word_tokenize(raw)\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)), \n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfBush = dfBush.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathTrump):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathTrump + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('-')\n",
    "        wordsNoPunct.replace(\"-\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        \n",
    "        words = word_tokenize(raw)\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)), \n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfTrump = dfTrump.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfReagan] \n",
    "        + [col for col in dfReagan if col not in inserted_cols])\n",
    "dfReagan = dfReagan[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfBush] \n",
    "        + [col for col in dfBush if col not in inserted_cols])\n",
    "dfBush = dfBush[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfTrump] \n",
    "        + [col for col in dfTrump if col not in inserted_cols])\n",
    "dfTrump = dfTrump[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA's with 0s as in this dataset, NAN represent the feature NOT occuring in a particular document.\n",
    "dfReagan = dfReagan.fillna(0)\n",
    "dfBush = dfBush.fillna(0)\n",
    "dfTrump = dfTrump.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan.to_csv('testReagan.csv')\n",
    "dfBush.to_csv('testBush.csv')\n",
    "dfTrump.to_csv('testTrump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac\n",
    "# testpath = '/Users/Joe/Documents/Coding/'\n",
    "# For Linux\n",
    "testpath = '/home/CAMPUS/alcantaj/Documents/Coding/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(testpath + \"dates.csv\")\n",
    "b = pd.read_csv(testpath + \"testReagan.csv\")\n",
    "\n",
    "c = pd.read_csv(testpath + \"dates2.csv\")\n",
    "d = pd.read_csv(testpath + \"testBush.csv\")\n",
    "\n",
    "e = pd.read_csv(testpath + \"dates3.csv\")\n",
    "f = pd.read_csv(testpath + \"testTrump.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in a['Date']]\n",
    "c['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in c['Date']]\n",
    "e['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in e['Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Julian'] = [get_julian_datetime(x) for x in a['JDate']]\n",
    "c['Julian'] = [get_julian_datetime(x) for x in c['JDate']]\n",
    "e['Julian'] = [get_julian_datetime(x) for x in e['JDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedRR = a.merge(b, on='Filename')\n",
    "mergedGWHB = c.merge(d, on='Filename')\n",
    "mergedDJT = e.merge(f, on='Filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedRR = mergedRR.sort_values(by=['JDate'])\n",
    "mergedGWHB = mergedGWHB.sort_values(by=['JDate'])\n",
    "mergedDJT = mergedDJT.sort_values(by=['JDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new labels (Index is in order of article date)\n",
    "mergedRR = mergedRR.reset_index()\n",
    "mergedRR['index'] = mergedRR.index\n",
    "mergedGWHB = mergedGWHB.reset_index()\n",
    "mergedGWHB['index'] = mergedGWHB.index\n",
    "mergedDJT = mergedDJT.reset_index()\n",
    "mergedDJT['index'] = mergedDJT.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Redundant Columns\n",
    "mergedRR = mergedRR.drop(['Unnamed: 0'], axis=1)\n",
    "mergedGWHB = mergedGWHB.drop(['Unnamed: 0'], axis=1)\n",
    "mergedDJT = mergedDJT.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid confusion, rename merged to df\n",
    "dfRR = mergedRR\n",
    "dfGWHB = mergedGWHB\n",
    "dfDJT = mergedDJT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfRR['Nouns'] = dfRR['NN'] + dfRR['NNS']+ dfRR['NNP'] + dfRR['NNPS']\n",
    "dfRR['Nouns/100'] = dfRR['Nouns'] / 100\n",
    "dfRR['NounsNormalised'] = dfRR['Nouns'] / dfRR['WordCount']\n",
    "dfRR['Adjectives'] = dfRR['JJ'] + dfRR['JJR'] + dfRR['JJS']\n",
    "dfRR['Adjectives/100'] = dfRR['Adjectives'] / 100\n",
    "dfRR['AdjectivesNormalised'] = dfRR['Adjectives'] / dfRR['WordCount']\n",
    "dfRR['Adverbs'] = dfRR['RB'] + dfRR['RBR'] + dfRR['RBS']\n",
    "dfRR['Adverbs/100'] = dfRR['Adverbs'] / 100\n",
    "dfRR['AdverbsNormalised'] = dfRR['Adverbs'] / dfRR['WordCount']\n",
    "dfRR['Verbs'] = dfRR['VB'] + dfRR['VBD'] + dfRR['VBG'] + dfRR['VBN'] + dfRR['VBP'] + dfRR['VBZ']\n",
    "dfRR['Verbs/100'] = dfRR['Verbs'] / 100\n",
    "dfRR['VerbsNormalised'] = dfRR['Verbs'] / dfRR['WordCount']\n",
    "dfRR['Pronouns'] = dfRR['PRP'] + dfRR['PRP$']\n",
    "dfRR['PronounsNormalised'] = dfRR['Pronouns'] / dfRR['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfGWHB['Nouns'] = dfGWHB['NN'] + dfGWHB['NNS']+ dfGWHB['NNP'] + dfGWHB['NNPS']\n",
    "dfGWHB['Nouns/100'] = dfGWHB['Nouns'] / 100\n",
    "dfGWHB['NounsNormalised'] = dfGWHB['Nouns'] / dfGWHB['WordCount']\n",
    "dfGWHB['Adjectives'] = dfGWHB['JJ'] + dfGWHB['JJR'] + dfGWHB['JJS']\n",
    "dfGWHB['Adjectives/100'] = dfGWHB['Adjectives'] / 100\n",
    "dfGWHB['AdjectivesNormalised'] = dfGWHB['Adjectives'] / dfGWHB['WordCount']\n",
    "dfGWHB['Adverbs'] = dfGWHB['RB'] + dfGWHB['RBR'] + dfGWHB['RBS']\n",
    "dfGWHB['Adverbs/100'] = dfGWHB['Adverbs'] / 100\n",
    "dfGWHB['AdverbsNormalised'] = dfGWHB['Adverbs'] / dfGWHB['WordCount']\n",
    "dfGWHB['Verbs'] = dfGWHB['VB'] + dfGWHB['VBD'] + dfGWHB['VBG'] + dfGWHB['VBN'] + dfGWHB['VBP'] + dfGWHB['VBZ']\n",
    "dfGWHB['Verbs/100'] = dfGWHB['Verbs'] / 100\n",
    "dfGWHB['VerbsNormalised'] = dfGWHB['Verbs'] / dfGWHB['WordCount']\n",
    "dfGWHB['Pronouns'] = dfGWHB['PRP'] + dfGWHB['PRP$']\n",
    "dfGWHB['PronounsNormalised'] = dfGWHB['Pronouns'] / dfGWHB['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfDJT['Nouns'] = dfDJT['NN'] + dfDJT['NNS']+ dfDJT['NNP'] + dfDJT['NNPS']\n",
    "dfDJT['Nouns/100'] = dfDJT['Nouns'] / 100\n",
    "dfDJT['NounsNormalised'] = dfDJT['Nouns'] / dfDJT['WordCount']\n",
    "dfDJT['Adjectives'] = dfDJT['JJ'] + dfDJT['JJR'] + dfDJT['JJS']\n",
    "dfDJT['Adjectives/100'] = dfDJT['Adjectives'] / 100\n",
    "dfDJT['AdjectivesNormalised'] = dfDJT['Adjectives'] / dfDJT['WordCount']\n",
    "dfDJT['Adverbs'] = dfDJT['RB'] + dfDJT['RBR'] + dfDJT['RBS']\n",
    "dfDJT['Adverbs/100'] = dfDJT['Adverbs'] / 100\n",
    "dfDJT['AdverbsNormalised'] = dfDJT['Adverbs'] / dfDJT['WordCount']\n",
    "dfDJT['Verbs'] = dfDJT['VB'] + dfDJT['VBD'] + dfDJT['VBG'] + dfDJT['VBN'] + dfDJT['VBP'] + dfDJT['VBZ']\n",
    "dfDJT['Verbs/100'] = dfDJT['Verbs'] / 100\n",
    "dfDJT['VerbsNormalised'] = dfDJT['Verbs'] / dfDJT['WordCount']\n",
    "dfDJT['Pronouns'] = dfDJT['PRP'] + dfDJT['PRP$']\n",
    "dfDJT['PronounsNormalised'] = dfDJT['Pronouns'] / dfRR['WordCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to split data here\n",
    "Logic\n",
    "For dfRR where Date is between 01/01/1981 and 31/01/1985 copy data into dfRRTerm1\n",
    "For dfRR where Date is not between 01/01/1981 and 31/01/1985 copy data into dfRRTerm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRRTerm1 = dfRR[dfRR.JDate < datetime.date(1985, 1, 31)]\n",
    "dfRRTerm2 = dfRR[dfRR.JDate > datetime.date(1985, 1, 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRRTerm1 = dfRRTerm1.reset_index()\n",
    "dfRRTerm1['index'] = dfRRTerm1.index\n",
    "dfRRTerm2 = dfRRTerm2.reset_index()\n",
    "dfRRTerm2['index'] = dfRRTerm2.index\n",
    "\n",
    "dfRRTerm1 = dfRRTerm1.drop(['level_0'], axis=1)\n",
    "dfRRTerm2 = dfRRTerm2.drop(['level_0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Target Variable\n",
    "yRR = dfRR['index']\n",
    "yRRTerm1 = dfRRTerm1['index']\n",
    "yRRTerm2 = dfRRTerm2['index']\n",
    "yGWHB = dfGWHB['index']\n",
    "yDJT = dfDJT['index']\n",
    "\n",
    "#yRR = dfRR['Julian']\n",
    "#yGWHB = dfGWHB['Julian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to csv files\n",
    "dfRR.to_csv('Reagan.csv')\n",
    "dfGWHB.to_csv('Bush.csv')\n",
    "dfDJT.to_csv('Trump.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RR Unique Words Mean', dfRR['UniqueWords'].mean())\n",
    "print('RR Unique Words SD', dfRR['UniqueWords'].std())\n",
    "print('RR Term 1 Unique Words Mean', dfRRTerm1['UniqueWords'].mean())\n",
    "print('RR Term 1 Unique Words SD', dfRRTerm1['UniqueWords'].std())\n",
    "print('RR Term 2 Unique Words Mean', dfRRTerm2['UniqueWords'].mean())\n",
    "print('RR Term 2 Unique Words SD', dfRRTerm2['UniqueWords'].std())\n",
    "print('GHWB Unique Words Mean', dfGWHB['UniqueWords'].mean())\n",
    "print('GWHB Unique Words SD', dfGWHB['UniqueWords'].std())\n",
    "print('DJT Unique Words Mean', dfDJT['UniqueWords'].mean())\n",
    "print('DJT Unique Words SD', dfDJT['UniqueWords'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Unique Words to GWHB Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRR['UniqueWords'], dfGWHB['UniqueWords'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Unique Words to DJT Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRR['UniqueWords'], dfDJT['UniqueWords'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing GWHB Unique Words to DJT Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfGWHB['UniqueWords'], dfDJT['UniqueWords'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 1 Unique Words to RR Term 2 Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm1['UniqueWords'], dfRRTerm2['UniqueWords'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 2 Unique Words to GWHB Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm2['UniqueWords'], dfGWHB['UniqueWords'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RR Non Specific Nouns Mean', dfRR['NSNouns'].mean())\n",
    "print('RR Non Specific Nouns SD', dfRR['NSNouns'].std())\n",
    "print('RR Term 1 Specific Nouns Mean', dfRRTerm1['NSNouns'].mean())\n",
    "print('RR Term 1 Specific Nouns SD', dfRRTerm1['NSNouns'].std())\n",
    "print('RR Term 2 Specific Nouns Mean', dfRRTerm2['NSNouns'].mean())\n",
    "print('RR Term 2 Specific Nouns SD', dfRRTerm2['NSNouns'].std())\n",
    "print('GHWB Non Specific Nouns Mean', dfGWHB['NSNouns'].mean())\n",
    "print('GWHB Non Specific Nouns SD', dfGWHB['NSNouns'].std())\n",
    "print('DJT Non Specific Nouns Mean', dfDJT['NSNouns'].mean())\n",
    "print('DJT Non Specific Nouns SD', dfDJT['NSNouns'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfRR['NSNouns'], dfGWHB['NSNouns'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfGWHB['NSNouns'], dfDJT['NSNouns'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfRR['NSNouns'], dfDJT['NSNouns'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 1 Unique Words to RR Term 2 Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm1['NSNouns'], dfRRTerm2['NSNouns'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 1 Unique Words to RR Term 2 Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm1['NSNouns'], dfRRTerm2['NSNouns'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RR Low Imageability Verbs Mean', dfRR['LIVerbs'].mean())\n",
    "print('RR Low Imageability Verbs SD', dfRR['LIVerbs'].std())\n",
    "print('RR Term 1 Low Imageability Verbs Mean', dfRRTerm1['LIVerbs'].mean())\n",
    "print('RR Term 1 Low Imageability Verbs SD', dfRRTerm1['LIVerbs'].std())\n",
    "print('RR Term 2 Low Imageability Verbs Mean', dfRRTerm2['LIVerbs'].mean())\n",
    "print('RR Term 2 Low Imageability Verbs SD', dfRRTerm2['LIVerbs'].std())\n",
    "print('GHWB Low Imageability Verbs Mean', dfGWHB['LIVerbs'].mean())\n",
    "print('GWHB Low Imageability Verbs SD', dfGWHB['LIVerbs'].std())\n",
    "print('DJT Low Imageability Verbs Mean', dfDJT['LIVerbs'].mean())\n",
    "print('DJT Low Imageability Verbs SD', dfDJT['LIVerbs'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfRR['LIVerbs'], dfGWHB['LIVerbs'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfGWHB['LIVerbs'], dfDJT['LIVerbs'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfDJT['LIVerbs'], dfRR['LIVerbs'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 1 Unique Words to RR Term 2 Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm1['LIVerbs'], dfRRTerm2['LIVerbs'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 2 Unique Words to GWHB Unique Words\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm2['LIVerbs'], dfGWHB['LIVerbs'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RR Total Word Count Mean', dfRR['WordCount'].mean())\n",
    "print('RR Total Word Count  SD', dfRR['WordCount'].std())\n",
    "print('RR Term 1 Word Count Mean', dfRRTerm1['WordCount'].mean())\n",
    "print('RR Term 1 Word Count SD', dfRRTerm1['WordCount'].std())\n",
    "print('RR Term 2 Word Count Mean', dfRRTerm2['WordCount'].mean())\n",
    "print('RR Term 2 Word Count SD', dfRRTerm2['WordCount'].std())\n",
    "print('GHWB Total Word Count Mean', dfGWHB['WordCount'].mean())\n",
    "print('GWHB Total Word Count SD', dfGWHB['WordCount'].std())\n",
    "print('DJT Total Word Count Mean', dfDJT['WordCount'].mean())\n",
    "print('DJT Total Word Count SD', dfDJT['WordCount'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfRR['WordCount'], dfGWHB['WordCount'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfGWHB['WordCount'], dfDJT['WordCount'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2, p2 = stats.ttest_ind(dfRR['WordCount'], dfDJT['WordCount'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 1 Word Count to RR Term 2 Word Count\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm1['WordCount'], dfRRTerm2['WordCount'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test comparing RR Term 1 Word Count to RR Term 2 Word Count\n",
    "t2, p2 = stats.ttest_ind(dfRRTerm2['WordCount'], dfGWHB['WordCount'])\n",
    "print('T Value', t2)\n",
    "print('P Value', '{0:.10f}'.format(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['UniqueWords'], dfRR['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['UniqueWords'], dfRRTerm1['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['UniqueWords'], dfRRTerm2['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['UniqueWords'], dfGWHB['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['UniqueWords'], dfDJT['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['NSNouns'], dfRR['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['NSNouns'], dfRRTerm1['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['NSNouns'], dfRRTerm2['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['NSNouns'], dfGWHB['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['NSNouns'], dfDJT['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['LIVerbs'], dfRR['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['LIVerbs'], dfRRTerm1['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['LIVerbs'], dfRRTerm2['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['LIVerbs'], dfGWHB['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['LIVerbs'], dfDJT['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['index'], (dfRR['NounsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['index'], (dfRRTerm1['NounsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['index'], (dfRRTerm2['NounsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['index'], (dfGWHB['NounsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['index'], (dfDJT['NounsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['index'], (dfRR['VerbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['index'], (dfRRTerm1['VerbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['index'], (dfRRTerm2['VerbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['index'], (dfGWHB['VerbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['index'], (dfDJT['VerbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['index'], (dfRR['AdjectivesNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['index'], (dfRRTerm1['AdjectivesNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['index'], (dfRRTerm2['AdjectivesNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['index'], (dfGWHB['AdjectivesNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['index'], (dfDJT['AdjectivesNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['index'], (dfRR['AdverbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm1['index'], (dfRRTerm1['AdverbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRRTerm2['index'], (dfRRTerm2['AdverbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['index'], (dfGWHB['AdverbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['index'], (dfDJT['AdverbsNormalised']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['PronounsNormalised'], dfRR['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['PronounsNormalised'], dfGWHB['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfDJT['PronounsNormalised'], dfDJT['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfRR['WordCount'], dfRR['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['WordCount'], dfGWHB['index'])\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = pearsonr(dfGWHB['index'], (dfGWHB['TTR']))\n",
    "print('R Squared', r)\n",
    "print('P Value', '{0:.10f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Length of Utterance over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['MLU']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Length of Utterance over time for GWHB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['MLU']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Length of Utterance over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['MLU']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unique Words over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['UniqueWords']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0, 1200))\n",
    "plt.suptitle('Ronald Reagan - Unique Words')\n",
    "plt.savefig('RRUniqueWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unique Words over time for GWHB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['UniqueWords']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.suptitle('George Bush Snr - Unique Words')\n",
    "plt.savefig('GWHBUniqueWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unique Words over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['UniqueWords']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Type Ratio over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['TTR']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Type Ratio over time for GWHB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['TTR']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Type Ratio over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['TTR']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['WordCount']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['WordCount']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['WordCount']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nouns per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Nouns/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nouns per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Nouns/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nouns per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Nouns/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Verbs per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Verbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Verbs per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Verbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Verbs per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Verbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adjectives per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Adjectives/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adjectives per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Adjectives/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adjectives per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Adjectives/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adverbs per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Adverbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adverbs per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Adverbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adverbs per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Adverbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['NounsNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('RR - Nouns Normalised')\n",
    "plt.savefig('RRNounsNorm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['NounsNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('GWHB - Nouns Normalised')\n",
    "plt.savefig('GWHBNounsNorm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['NounsNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('DJT - Nouns Normalised')\n",
    "plt.savefig('DJTNounsNorm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['AdjectivesNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['AdjectivesNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['AdjectivesNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['Fillers']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Fillers']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Fillers']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['NSNouns']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('Ronald Reagan - Non Specific Nouns')\n",
    "plt.savefig('RRNSNouns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['NSNouns']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['NSNouns']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('Donald Trump - Non Specific Nouns')\n",
    "plt.savefig('DJTNSNouns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['LIVerbs']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['LIVerbs']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['LIVerbs']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsRR = dfRR['Filename']\n",
    "dfRR = dfRR.drop(['Filename', 'JDate', 'Julian', 'Date', 'index'], axis=1)\n",
    "labelsGHWB = dfGWHB['Filename']\n",
    "dfGWHB = dfGWHB.drop(['Filename', 'JDate', 'Julian', 'Date', 'index'], axis=1)\n",
    "labelsDJT = dfDJT['Filename']\n",
    "dfDJT = dfDJT.drop(['Filename', 'JDate', 'Julian', 'Date', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "yRRscaled = preprocessing.scale(yRR)\n",
    "dfRRscaled = preprocessing.scale(dfRR)\n",
    "\n",
    "yGWHBscaled = preprocessing.scale(yGWHB)\n",
    "dfGWHBscaled = preprocessing.scale(dfGWHB)\n",
    "\n",
    "yDJTscaled = preprocessing.scale(yDJT)\n",
    "dfDJTscaled = preprocessing.scale(dfDJT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- LINEAR REGRESSION --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridRR = pd.DataFrame()\n",
    "for i in range (1,1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfRRscaled, yRR, test_size=0.20)\n",
    "    # fit a model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(predictions, y_test)\n",
    "    resultsTuple = {'Slope': slope, 'intercept': intercept, 'r_value': r_value,\n",
    "                    'p_value':p_value, \n",
    "                    'std_err':std_err}\n",
    "    resultsGridRR = resultsGridRR.append(resultsTuple, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridGWHB = pd.DataFrame()\n",
    "for i in range (1,1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfGWHBscaled, yGWHB, test_size=0.20)\n",
    "    # fit a model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(predictions, y_test)\n",
    "    resultsTuple = {'Slope': slope, 'intercept': intercept, 'r_value': r_value,\n",
    "                    'p_value':p_value, \n",
    "                    'std_err':std_err}\n",
    "    resultsGridGWHB = resultsGridGWHB.append(resultsTuple, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridDJT = pd.DataFrame()\n",
    "for i in range (1,1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfDJTscaled, yDJT, test_size=0.20)\n",
    "    # fit a model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(predictions, y_test)\n",
    "    resultsTuple = {'Slope': slope, 'intercept': intercept, 'r_value': r_value,\n",
    "                    'p_value':p_value, \n",
    "                    'std_err':std_err}\n",
    "    resultsGridDJT = resultsGridDJT.append(resultsTuple, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridRR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridGWHB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridDJT.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- GAUSSIAN PROCESSES --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/CAMPUS/alcantaj/Dropbox/'\n",
    "path = '/Users/Joe/Documents/Coding/'\n",
    "bush_df = pd.read_csv(path + 'Bush.csv')\n",
    "reagan_df = pd.read_csv(path + 'Reagan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = reagan_df['Julian'].min() # Smallest Julian Date\n",
    "maximum = reagan_df['Julian'].max() # Maximum Julian Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['NormalisedDate'] = (reagan_df['Julian'] - minimum) / (maximum - minimum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDays'] = reagan_df['Julian'] - minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDaysN'] = reagan_df['CountDays'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO DROP DATA COLUMNS HERE BEFORE RUN MODEL\n",
    "reagan_df = reagan_df.drop(['Unnamed: 0', 'Filename', 'index','Date', 'JDate', 'Julian',\n",
    "                           'NormalisedDate', 'CountDays'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reagan_df['CountDaysN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target = target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(185, (1e-2, 1e4))\n",
    "# kernel = RBF(10, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(reagan_df, target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.asarray(y_train)\n",
    "variables = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "model = gp.fit(variables, target) # This looks fine according to documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigma = gp.predict(np.asarray(X_test), return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot\n",
    "plt.figure()\n",
    "xs = [x for x in range(0, 9)]\n",
    "ys = [x for x in range(0, 9)]\n",
    "plt.plot(xs, ys)\n",
    "plt.plot(y_test, y_pred, 'o', linestyle='None')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis(xscale = 0, yscale = 0)\n",
    "plt.errorbar(y_test, y_pred, yerr=1.9600 * sigma, elinewidth=1, fillstyle='full', linestyle ='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqErr = (y_test - y_pred)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y_test':y_test, 'y_pred':y_pred, 'sigma': sigma, 'Squared Error': sqErr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = sum(sqErr) / len(sqErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_df = pd.read_csv(path + 'Bush.csv')\n",
    "reagan_df = pd.read_csv(path + 'Reagan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = reagan_df['Julian'].min() # Smallest Julian Date\n",
    "maximum = reagan_df['Julian'].max() # Maximum Julian Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['NormalisedDate'] = (reagan_df['Julian'] - minimum) / (maximum - minimum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDays'] = reagan_df['Julian'] - minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDaysN'] = reagan_df['CountDays'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO DROP DATA COLUMNS HERE BEFORE RUN MODEL\n",
    "reagan_df = reagan_df.drop(['Unnamed: 0', 'Filename', 'index','Date', 'JDate', 'Julian',\n",
    "                           'NormalisedDate', 'CountDays'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reagan_df['CountDaysN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target = target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(191, (1e-2, 1e4))\n",
    "# kernel = RBF(10, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reagan_df, target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.DataFrame()\n",
    "for i in range(0, 999):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reagan_df, target, test_size=0.20)\n",
    "    targetVar = np.asarray(y_train)\n",
    "    variables = np.asarray(X_train)\n",
    "    model = gp.fit(variables, targetVar)\n",
    "    y_pred, sigma = gp.predict(np.asarray(X_test), return_std=True)\n",
    "    sqErr = (y_test - y_pred)**2\n",
    "    MSE = sum(sqErr)/len(sqErr)\n",
    "    tuple = {'MSE': MSE, 'gp.kernel':gp.kernel, 'gp.kernel_':gp.kernel_}\n",
    "    Results = Results.append(tuple, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean = Results['MSE'].mean()\n",
    "SD = Results['MSE'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Mean is', Mean)\n",
    "print('The SD is', SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
